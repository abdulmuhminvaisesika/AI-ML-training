{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e85f09fd-4752-4cfa-8929-ff588de62b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8c4829c-f13e-41bf-984e-eba095b60996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abdul.muhmin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abdul.muhmin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abdul.muhmin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\abdul.muhmin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9df67183-367e-4336-bf32-53b779b340c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "def load_data(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9aef5c30-15fc-4609-9b24-8fc82433b1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map POS tags to WordNet format\n",
    "def get_wordnet_pos(nltk_pos):\n",
    "    if nltk_pos.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_pos.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_pos.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_pos.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default noun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24837d68-ef02-4625-92c1-1f0f8ac3d713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and Sequence Generation\n",
    "def preprocess_text(text, sequence_length=5):\n",
    "\n",
    "\n",
    "    # Tokenization\n",
    "    tokenized_words = word_tokenize(text)\n",
    "\n",
    "    # Initialize Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Get Stopwords and Punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "\n",
    "    # Apply stopwords, punctuation removal and lemmatization\n",
    "    words = [\n",
    "        lemmatizer.lemmatize(word.lower(), get_wordnet_pos(pos))\n",
    "        for word, pos in pos_tag(tokenized_words)\n",
    "        if word not in stop_words and word not in punctuation\n",
    "    ]\n",
    "    \n",
    "    word_counts = Counter(words)\n",
    "    vocab = {word: i+1 for i, (word, _) in enumerate(word_counts.most_common())}\n",
    "    vocab['<UNK>'] = len(vocab) + 1\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    sequences = []\n",
    "    for i in range(len(words) - sequence_length):\n",
    "        seq = words[i: i + sequence_length]\n",
    "        sequences.append([ vocab.get(word, vocab['<UNK>']) for word in seq])\n",
    "\n",
    "    return sequences, vocab, vocab_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e65ccf8-3cac-43ab-8b96-0a4e3151f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom Dataset class\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.x = torch.tensor([seq[:-1] for seq in sequences], dtype=torch.long)\n",
    "        self.y = torch.tensor([seq[-1] for seq in sequences], dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5f6808c-3998-4453-bab1-b5a882e4d295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4adeae-32f3-4d2f-ab0a-65282a260fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train_model(model, dataloader, vocab_size, epochs=10, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09e5572c-cfb0-4b8c-b9c4-5bf4a21b7446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Function\n",
    "def predict(model, text, vocab, sequence_length=5):\n",
    "    model.eval() #\n",
    "    words = word_tokenize(text.lower())\n",
    "    sequence = [vocab.get(word, vocab['<UNK>']) for word in words][-sequence_length:]\n",
    "    input_tensor = torch.tensor(sequence, dtype=torch.long).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        predicted_idx = torch.argmax(output, dim=1).item()\n",
    "    \n",
    "    inv_vocab = {idx: word for word, idx in vocab.items()}\n",
    "    return inv_vocab.get(predicted_idx, '<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "647632a0-ad24-4744-93b9-c33b67234467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution\n",
    "filepath = r\"C:\\Users\\abdul.muhmin\\Downloads\\Sherlock_Holmes.txt\"\n",
    "text = load_data(filepath)\n",
    "sequences, vocab, vocab_size = preprocess_text(text)\n",
    "dataset = TextDataset(sequences)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11e6ba50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.2798\n",
      "Epoch 2, Loss: 6.5072\n",
      "Epoch 3, Loss: 5.5631\n",
      "Epoch 4, Loss: 4.5442\n",
      "Epoch 5, Loss: 3.6782\n",
      "Epoch 6, Loss: 2.9872\n",
      "Epoch 7, Loss: 2.4161\n",
      "Epoch 8, Loss: 1.9297\n",
      "Epoch 9, Loss: 1.5149\n",
      "Epoch 10, Loss: 1.1642\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train model\n",
    "model = LSTMModel(vocab_size)\n",
    "train_model(model, dataloader, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53e8c338-8089-488c-912e-84b0f75ee994",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Test Prediction\u001b[39;00m\n\u001b[0;32m      2\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Man with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m predicted_word \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted completion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_word\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 8\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(model, text, vocab, sequence_length)\u001b[0m\n\u001b[0;32m      6\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(sequence, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 8\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     predicted_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     11\u001b[0m inv_vocab \u001b[38;5;241m=\u001b[39m {idx: word \u001b[38;5;28;01mfor\u001b[39;00m word, idx \u001b[38;5;129;01min\u001b[39;00m vocab\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[1;32mc:\\Users\\abdul.muhmin\\AppData\\Local\\anaconda3\\envs\\py310_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\abdul.muhmin\\AppData\\Local\\anaconda3\\envs\\py310_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[17], line 10\u001b[0m, in \u001b[0;36mLSTMModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x)\n\u001b[0;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n",
      "File \u001b[1;32mc:\\Users\\abdul.muhmin\\AppData\\Local\\anaconda3\\envs\\py310_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\abdul.muhmin\\AppData\\Local\\anaconda3\\envs\\py310_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\abdul.muhmin\\AppData\\Local\\anaconda3\\envs\\py310_env\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abdul.muhmin\\AppData\\Local\\anaconda3\\envs\\py310_env\\lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test Prediction\n",
    "input_text = \"The Man with\"\n",
    "predicted_word = predict(model, input_text, vocab)\n",
    "print(f\"Predicted completion: {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "466b6b54-015a-42e2-bc56-750e3dd148ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abdul.muhmin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abdul.muhmin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abdul.muhmin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\abdul.muhmin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.2657\n",
      "Epoch 2, Loss: 6.5086\n",
      "Epoch 3, Loss: 5.5908\n",
      "Epoch 4, Loss: 4.5808\n",
      "Epoch 5, Loss: 3.7023\n",
      "Epoch 6, Loss: 3.0016\n",
      "Epoch 7, Loss: 2.4280\n",
      "Epoch 8, Loss: 1.9403\n",
      "Epoch 9, Loss: 1.5229\n",
      "Epoch 10, Loss: 1.1711\n",
      "Tokenized Input: ['the', 'man', 'with']\n",
      "Filtered Words: ['man']\n",
      "Mapped Sequence: [10]\n",
      "Input Tensor Shape: torch.Size([1, 1])\n",
      "Predicted completion: 's\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Load dataset\n",
    "def load_data(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().lower()\n",
    "    return text\n",
    "\n",
    "# Function to map POS tags to WordNet format\n",
    "def get_wordnet_pos(nltk_pos):\n",
    "    if nltk_pos.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_pos.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_pos.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_pos.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default noun\n",
    "\n",
    "# Tokenization and Sequence Generation\n",
    "def preprocess_text(text, sequence_length=5):\n",
    "    # Tokenization\n",
    "    tokenized_words = word_tokenize(text)\n",
    "\n",
    "    # Initialize Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Get Stopwords and Punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "\n",
    "    # Apply stopwords, punctuation removal, and lemmatization\n",
    "    words = [\n",
    "        lemmatizer.lemmatize(word.lower(), get_wordnet_pos(pos))\n",
    "        for word, pos in pos_tag(tokenized_words)\n",
    "        if word not in stop_words and word not in punctuation\n",
    "    ]\n",
    "    \n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    # Fix: Ensure vocabulary indexing starts from 0\n",
    "    vocab = {word: i for i, (word, _) in enumerate(word_counts.most_common())}\n",
    "    vocab['<UNK>'] = len(vocab)  # Last index\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    sequences = []\n",
    "    for i in range(len(words) - sequence_length):\n",
    "        seq = words[i: i + sequence_length]\n",
    "        sequences.append([vocab.get(word, vocab['<UNK>']) for word in seq])\n",
    "\n",
    "    return sequences, vocab, vocab_size\n",
    "\n",
    "# Custom Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.x = torch.tensor([seq[:-1] for seq in sequences], dtype=torch.long)\n",
    "        self.y = torch.tensor([seq[-1] for seq in sequences], dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "# LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :])  # Take last output of LSTM\n",
    "        return x\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, dataloader, vocab_size, epochs=10, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# Prediction Function\n",
    "def predict(model, text, vocab, sequence_length=5):\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize input text\n",
    "    words = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove words not in vocab\n",
    "    filtered_words = [word for word in words if word in vocab]\n",
    "    if not filtered_words:\n",
    "        print(\"Error: None of the words in input_text exist in the vocabulary!\")\n",
    "        return \"<UNK>\"\n",
    "\n",
    "    # Convert words to indices\n",
    "    sequence = [vocab.get(word, vocab['<UNK>']) for word in filtered_words][-sequence_length:]\n",
    "    \n",
    "    # Debugging outputs\n",
    "    print(\"Tokenized Input:\", words)\n",
    "    print(\"Filtered Words:\", filtered_words)\n",
    "    print(\"Mapped Sequence:\", sequence)\n",
    "    \n",
    "    input_tensor = torch.tensor(sequence, dtype=torch.long).unsqueeze(0)\n",
    "    print(\"Input Tensor Shape:\", input_tensor.shape)  # Ensure correct shape\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        predicted_idx = torch.argmax(output, dim=1).item()\n",
    "    \n",
    "    inv_vocab = {idx: word for word, idx in vocab.items()}\n",
    "    return inv_vocab.get(predicted_idx, '<UNK>')\n",
    "\n",
    "# Main Execution\n",
    "filepath = r\"C:\\Users\\abdul.muhmin\\Downloads\\Sherlock_Holmes.txt\"\n",
    "text = load_data(filepath)\n",
    "sequences, vocab, vocab_size = preprocess_text(text)\n",
    "dataset = TextDataset(sequences)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize and train model\n",
    "model = LSTMModel(vocab_size)\n",
    "train_model(model, dataloader, vocab_size)\n",
    "\n",
    "# Test Prediction\n",
    "input_text = \"The Man with\"\n",
    "predicted_word = predict(model, input_text, vocab)\n",
    "print(f\"Predicted completion: {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8fcbb2e7-2e82-4ccc-b380-74cf19dfec77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Input: ['the', 'adventures', 'of', 'sherlock']\n",
      "Filtered Words: ['sherlock']\n",
      "Mapped Sequence: [62]\n",
      "Input Tensor Shape: torch.Size([1, 1])\n",
      "Predicted completion: holmes\n"
     ]
    }
   ],
   "source": [
    "# Test Prediction\n",
    "input_text = \"THE ADVENTURES OF SHERLOCK \"\n",
    "predicted_word = predict(model, input_text, vocab)\n",
    "print(f\"Predicted completion: {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c20176-6903-47a8-9cc3-cb045432e3ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8412e946-784e-4b2b-9181-8e3c908a4935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8972010-b9b6-495c-befc-ff131db58800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6725138-d29f-453a-aaab-67e90941cf51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dc3f54-0fdd-4da2-a727-7e7729af27c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8386e49-86e7-43ab-aa91-b550c5e47a65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3441630f-de9c-48b0-9154-77209ff68bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db4281d-bada-4f70-ad7f-a2137f6ca6d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419b0657-a6b1-4121-a9e1-399351ab1a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4747121-b7a7-4993-8fe4-6c53130363dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_env",
   "language": "python",
   "name": "py310_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
