{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abdul.muhmin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abdul.muhmin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abdul.muhmin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\abdul.muhmin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "def load_data(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map POS tags to WordNet format\n",
    "def get_wordnet_pos(nltk_pos):\n",
    "    if nltk_pos.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_pos.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_pos.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_pos.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default noun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and Sequence Generation\n",
    "def preprocess_text(text, sequence_length=5):\n",
    "\n",
    "    \n",
    "    # Remove special characters using regex\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Keep only words and spaces\n",
    "\n",
    "    # Tokenization\n",
    "    tokenized_words = word_tokenize(text)\n",
    "\n",
    "    # Initialize Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Get Stopwords and Punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "\n",
    "    # Apply stopwords, punctuation removal, and lemmatization\n",
    "    words = [\n",
    "        lemmatizer.lemmatize(word.lower(), get_wordnet_pos(pos))\n",
    "        for word, pos in pos_tag(tokenized_words)\n",
    "        if word not in stop_words and word not in punctuation\n",
    "    ]\n",
    "    \n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    # Fix: Ensure vocabulary indexing starts from 0\n",
    "    vocab = {word: i for i, (word, _) in enumerate(word_counts.most_common())}\n",
    "    vocab['<UNK>'] = len(vocab)  # Last index\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    sequences = []\n",
    "    for i in range(len(words) - sequence_length):\n",
    "        seq = words[i: i + sequence_length]\n",
    "        sequences.append([vocab.get(word, vocab['<UNK>']) for word in seq])\n",
    "\n",
    "    return sequences, vocab, vocab_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.x = torch.tensor([seq[:-1] for seq in sequences], dtype=torch.long)\n",
    "        self.y = torch.tensor([seq[-1] for seq in sequences], dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :])  # Take last output of LSTM\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train_model(model, dataloader, vocab_size, epochs=25, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_pred = model(x_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prediction Function\n",
    "def predict(model, text, vocab, sequence_length=5):\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize input text\n",
    "    words = word_tokenize(text.lower())\n",
    "\n",
    "    #  Remove words not in vocab\n",
    "    filtered_words = [word for word in words if word in vocab]\n",
    "    if not filtered_words:\n",
    "       print(\"Error: None of the words in input_text exist in the vocabulary!\")\n",
    "       return \"<UNK>\"\n",
    "\n",
    "    # Convert words to indices\n",
    "    sequence = [vocab.get(word, vocab['<UNK>']) for word in filtered_words][-sequence_length:]\n",
    "    \n",
    "    \n",
    "    # Debugging outputs\n",
    "    print(\"Tokenized Input:\", words)\n",
    "    print(\"Filtered Words:\", filtered_words)\n",
    "    print(\"Mapped Sequence:\", sequence)\n",
    "    \n",
    "    input_tensor = torch.tensor(sequence, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        print(output.shape)\n",
    "        predicted_idx = torch.argmax(output, dim=1).item()\n",
    "    \n",
    "    inv_vocab = {idx: word for word, idx in vocab.items()}\n",
    "    return inv_vocab.get(predicted_idx, '<UNK>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution\n",
    "filepath = r\"C:\\Users\\abdul.muhmin\\Downloads\\Sherlock_Holmes.txt\"\n",
    "text = load_data(filepath)\n",
    "sequences, vocab, vocab_size = preprocess_text(text)\n",
    "dataset = TextDataset(sequences)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.7867\n",
      "Epoch 2, Loss: 6.9681\n",
      "Epoch 3, Loss: 5.8078\n",
      "Epoch 4, Loss: 4.6313\n",
      "Epoch 5, Loss: 3.6731\n",
      "Epoch 6, Loss: 2.9239\n",
      "Epoch 7, Loss: 2.3086\n",
      "Epoch 8, Loss: 1.7866\n",
      "Epoch 9, Loss: 1.3405\n",
      "Epoch 10, Loss: 0.9693\n",
      "Epoch 11, Loss: 0.6691\n",
      "Epoch 12, Loss: 0.4396\n",
      "Epoch 13, Loss: 0.2727\n",
      "Epoch 14, Loss: 0.1634\n",
      "Epoch 15, Loss: 0.0981\n",
      "Epoch 16, Loss: 0.0641\n",
      "Epoch 17, Loss: 0.0460\n",
      "Epoch 18, Loss: 0.0364\n",
      "Epoch 19, Loss: 0.0332\n",
      "Epoch 20, Loss: 0.0318\n",
      "Epoch 21, Loss: 0.0351\n",
      "Epoch 22, Loss: 0.0247\n",
      "Epoch 23, Loss: 0.0178\n",
      "Epoch 24, Loss: 0.0450\n",
      "Epoch 25, Loss: 0.0876\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train model\n",
    "model = LSTMModel(vocab_size)\n",
    "train_model(model, dataloader, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Input: ['the', 'adventures', 'of', 'sherlock']\n",
      "Filtered Words: ['sherlock']\n",
      "Mapped Sequence: [57]\n",
      "torch.Size([1, 6705])\n",
      "Predicted completion: holmes\n"
     ]
    }
   ],
   "source": [
    "# Test Prediction\n",
    "input_text = \"THE ADVENTURES OF SHERLOCK \"\n",
    "predicted_word = predict(model, input_text, vocab)\n",
    "print(f\"Predicted completion: {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Input: ['the', 'adventure', 'of', 'the', 'noble']\n",
      "Filtered Words: ['adventure', 'noble']\n",
      "Mapped Sequence: [349, 558]\n",
      "torch.Size([1, 6705])\n",
      "Predicted completion: bachelor\n"
     ]
    }
   ],
   "source": [
    "# Test Prediction\n",
    "input_text = \"The Adventure of the Noble \"\n",
    "predicted_word = predict(model, input_text, vocab)\n",
    "print(f\"Predicted completion: {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_env",
   "language": "python",
   "name": "py310_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
