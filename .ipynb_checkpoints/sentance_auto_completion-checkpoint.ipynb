{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e85f09fd-4752-4cfa-8929-ff588de62b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f8c4829c-f13e-41bf-984e-eba095b60996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abdul.muhmin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abdul.muhmin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abdul.muhmin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\abdul.muhmin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9df67183-367e-4336-bf32-53b779b340c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "def load_data(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9aef5c30-15fc-4609-9b24-8fc82433b1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map POS tags to WordNet format\n",
    "def get_wordnet_pos(nltk_pos):\n",
    "    if nltk_pos.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_pos.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_pos.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_pos.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default noun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "24837d68-ef02-4625-92c1-1f0f8ac3d713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and Sequence Generation\n",
    "def preprocess_text(text, sequence_length=5):\n",
    "\n",
    "\n",
    "    # Tokenization\n",
    "    tokenized_words = word_tokenize(text)\n",
    "\n",
    "    # Initialize Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Get Stopwords and Punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "\n",
    "    # Apply stopwords, punctuation removal and lemmatization\n",
    "    words = [\n",
    "        lemmatizer.lemmatize(word.lower(), get_wordnet_pos(pos))\n",
    "        for word, pos in pos_tag(tokenized_words)\n",
    "        if word not in stop_words and word not in punctuation\n",
    "    ]\n",
    "    \n",
    "    word_counts = Counter(words)\n",
    "    vocab = {word: i+1 for i, (word, _) in enumerate(word_counts.most_common())}\n",
    "    vocab['<UNK>'] = len(vocab) + 1\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    sequences = []\n",
    "    for i in range(len(words) - sequence_length):\n",
    "        seq = words[i: i + sequence_length]\n",
    "        sequences.append([ vocab.get(word, vocab['<UNK>']) for word in seq])\n",
    "\n",
    "    return sequences, vocab, vocab_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65ccf8-3cac-43ab-8b96-0a4e3151f36d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6808c-3998-4453-bab1-b5a882e4d295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4adeae-32f3-4d2f-ab0a-65282a260fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e5572c-cfb0-4b8c-b9c4-5bf4a21b7446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647632a0-ad24-4744-93b9-c33b67234467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e8c338-8089-488c-912e-84b0f75ee994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466b6b54-015a-42e2-bc56-750e3dd148ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcbb2e7-2e82-4ccc-b380-74cf19dfec77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c20176-6903-47a8-9cc3-cb045432e3ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8412e946-784e-4b2b-9181-8e3c908a4935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8972010-b9b6-495c-befc-ff131db58800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6725138-d29f-453a-aaab-67e90941cf51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dc3f54-0fdd-4da2-a727-7e7729af27c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8386e49-86e7-43ab-aa91-b550c5e47a65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3441630f-de9c-48b0-9154-77209ff68bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db4281d-bada-4f70-ad7f-a2137f6ca6d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419b0657-a6b1-4121-a9e1-399351ab1a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4747121-b7a7-4993-8fe4-6c53130363dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_env",
   "language": "python",
   "name": "py310_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
